{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-02T06:33:54.030946Z",
     "start_time": "2026-01-02T06:29:12.756198Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# --- Configuration ---\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1400)\n",
    "\n",
    "DATA_PATH = \"../data/processed/iyzico_featured_leakfree.csv\"\n",
    "OUT_PATH = \"../data/processed/ablation_results.csv\"\n",
    "RANDOM_STATE = 42\n",
    "TARGET = \"is_fraud_transaction\"\n",
    "MISSING_CAT = \"MISSING\"\n",
    "\n",
    "# Model settings for testing\n",
    "# We use fewer iterations here (600) to compare feature sets quickly\n",
    "PARAMS = dict(\n",
    "    iterations=600,\n",
    "    depth=8,\n",
    "    learning_rate=0.01,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    random_seed=RANDOM_STATE,\n",
    "    verbose=0,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "def time_split(df, train_q=0.70, valid_q=0.85):\n",
    "    \"\"\"Split data based on time order (Train -> Valid -> Test).\"\"\"\n",
    "    df = df.sort_values(\"payment_date\").reset_index(drop=True)\n",
    "\n",
    "    # Find cut-off dates\n",
    "    t0 = df[\"payment_date\"].quantile(train_q)\n",
    "    t1 = df[\"payment_date\"].quantile(valid_q)\n",
    "\n",
    "    # Split into 3 parts\n",
    "    train = df[df[\"payment_date\"] <= t0].copy()\n",
    "    valid = df[(df[\"payment_date\"] > t0) & (df[\"payment_date\"] <= t1)].copy()\n",
    "    test  = df[df[\"payment_date\"] > t1].copy()\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "def train_and_evaluate(train_df, valid_df, test_df, feature_cols, exp_name):\n",
    "    \"\"\"Train the model with specific features and return scores.\"\"\"\n",
    "\n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df[TARGET].astype(int)\n",
    "\n",
    "    X_valid = valid_df[feature_cols].copy()\n",
    "    y_valid = valid_df[TARGET].astype(int)\n",
    "\n",
    "    X_test  = test_df[feature_cols].copy()\n",
    "    y_test  = test_df[TARGET].astype(int)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    cat_features = [c for c in X_train.columns if str(X_train[c].dtype) in [\"object\", \"string\", \"category\"]]\n",
    "\n",
    "    # Fill missing values for categories\n",
    "    for c in cat_features:\n",
    "        X_train[c] = X_train[c].fillna(MISSING_CAT).astype(str)\n",
    "        X_valid[c] = X_valid[c].fillna(MISSING_CAT).astype(str)\n",
    "        X_test[c]  = X_test[c].fillna(MISSING_CAT).astype(str)\n",
    "\n",
    "    # Handle class imbalance (fraud is rare)\n",
    "    pos_count = y_train.sum()\n",
    "    neg_count = len(y_train) - pos_count\n",
    "    scale_pos_weight = neg_count / max(pos_count, 1)\n",
    "\n",
    "    model = CatBoostClassifier(**PARAMS, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    valid_pool = Pool(X_valid, y_valid, cat_features=cat_features)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50)\n",
    "\n",
    "    # Get predictions\n",
    "    preds_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUC scores\n",
    "    roc = roc_auc_score(y_test, preds_proba)\n",
    "    pr_auc = average_precision_score(y_test, preds_proba)\n",
    "\n",
    "    # Calculate Top 1% Recall (Business Metric)\n",
    "    k = max(int(len(y_test) * 0.01), 1)\n",
    "    top_k_idx = np.argsort(-preds_proba)[:k]\n",
    "    total_fraud = max(y_test.sum(), 1)\n",
    "    top_recall = y_test.iloc[top_k_idx].sum() / total_fraud\n",
    "\n",
    "    return {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Feat_Count\": len(feature_cols),\n",
    "        \"ROC_AUC\": round(roc, 4),\n",
    "        \"PR_AUC\": round(pr_auc, 4),\n",
    "        \"Top1%_Recall\": round(top_recall, 4)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "    # Convert date column\n",
    "    df[\"payment_date\"] = pd.to_datetime(df[\"payment_date\"])\n",
    "\n",
    "    # Split data\n",
    "    train, valid, test = time_split(df)\n",
    "\n",
    "    # --- Select Features ---\n",
    "    # Identify engineered features by keywords\n",
    "    velocity_keywords = [\"_cnt_\", \"_sum_\", \"_avg_\", \"_price_to_avg_\", \"_tsl_\"]\n",
    "    all_cols = [c for c in df.columns if c not in [TARGET, \"payment_date\", \"is_fraud_transaction\"]]\n",
    "\n",
    "    # 1. Raw Features only (No engineered features)\n",
    "    raw_feats = [c for c in all_cols if not any(k in c for k in velocity_keywords)]\n",
    "\n",
    "    # 2. Card Velocity only (Add card features, exclude others)\n",
    "    excluded_entities = [\"buyer_gsm\", \"merchant_id\", \"buyer_email\", \"payment_source_id\"]\n",
    "\n",
    "    card_velocity_feats = []\n",
    "    for c in all_cols:\n",
    "        is_velocity = any(k in c for k in velocity_keywords)\n",
    "\n",
    "        if not is_velocity:\n",
    "            card_velocity_feats.append(c) # Keep raw features\n",
    "        else:\n",
    "            # Keep only card-related velocity\n",
    "            if c.startswith(\"card_id\") and not any(c.startswith(e) for e in excluded_entities):\n",
    "                card_velocity_feats.append(c)\n",
    "\n",
    "    # 3. Full Model (Use everything)\n",
    "    full_feats = all_cols\n",
    "\n",
    "    # --- Run Experiments ---\n",
    "    results = []\n",
    "\n",
    "    # Exp A: Baseline\n",
    "    results.append(train_and_evaluate(train, valid, test, raw_feats, \"A) Baseline (Raw Only)\"))\n",
    "\n",
    "    # Exp B: Single Entity\n",
    "    results.append(train_and_evaluate(train, valid, test, card_velocity_feats, \"B) Card Velocity Only\"))\n",
    "\n",
    "    # Exp C: Full Model\n",
    "    results.append(train_and_evaluate(train, valid, test, full_feats, \"C) Full Multi-Entity\"))\n",
    "\n",
    "    # --- Show Results ---\n",
    "    res_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== ABLATION STUDY RESULTS ===\")\n",
    "    print(res_df.to_string(index=False))\n",
    "\n",
    "    res_df.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ABLATION STUDY RESULTS ===\n",
      "            Experiment  Feat_Count  ROC_AUC  PR_AUC  Top1%_Recall\n",
      "A) Baseline (Raw Only)          44   0.9811  0.0364        0.4472\n",
      " B) Card Velocity Only          54   0.9469  0.0210        0.0369\n",
      "  C) Full Multi-Entity          68   0.9826  0.0399        0.4988\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "43a6c7d410e1682a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
